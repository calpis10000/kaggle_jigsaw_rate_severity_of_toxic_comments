{"cells":[{"cell_type":"markdown","metadata":{"id":"91pntadwaltE"},"source":["\n","\n","# Jigsaw Rate Severity of Toxic Comments"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":431,"status":"ok","timestamp":1643244279835,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"afLlXS5HxnGq","outputId":"f561e4e2-0364-4f32-c4a6-991bbf5d39af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Jan 27 00:44:39 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    44W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1643244279836,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"KiUaJ_AjkgHd"},"outputs":[],"source":["import os\n","import pathlib\n","from pathlib import Path\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1643244279836,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"QbSjR-zQELvG"},"outputs":[],"source":["USERID = 'calpis10000'\n","EX_NO = 'jigsaw-calpis-001'\n","UPLOAD_DIR = Path('/content/model')\n","UPLOAD_DIR.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":895,"status":"ok","timestamp":1643244280726,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"B0z1220pA2gm"},"outputs":[],"source":["out_path = Path(f\"../output/{EX_NO}\")\n","out_path.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":66841,"status":"ok","timestamp":1643244347565,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"UIXTc5kPayUt"},"outputs":[],"source":["# copy input\n","!cp -r -f \"../input/\" \".\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27836,"status":"ok","timestamp":1643244375382,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"EEcNh7FTvdeL","outputId":"3ed89e90-5bbf-461b-90ac-d565e36ec580"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 5.1 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 71.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 73.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 78.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"]}],"source":["# install librariess\n","!pip install transformers"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7362,"status":"ok","timestamp":1643244382736,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"apbpcmylulBP"},"outputs":[],"source":["# ----------------------------------------------\n","# Path\n","# ----------------------------------------------\n","import pathlib\n","from pathlib import Path\n","import sys\n","\n","INPUT_DIR_0 = Path('./input/jigsaw-toxic-severity-rating/')\n","INPUT_DIR_1 = Path('./input/jigsaw-toxic-comment-classification-challenge/')\n","INPUT_DIR_2 = Path('./input/jigsaw-unintended-bias-in-toxicity-classification/')\n","\n","\n","# ----------------------------------------------\n","# Load Libraries\n","# ----------------------------------------------\n","import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import BertForSequenceClassification, BertConfig, BertModel\n","from transformers import get_cosine_schedule_with_warmup\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","\n","import gc\n","gc.enable()\n","\n","\n","# ----------------------------------------------\n","# Set Globals\n","# ----------------------------------------------\n","FOLDS = 5\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 3\n","MAX_LEN = 128\n","LEANING_RATE = 1e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # cudaがなければcpuを使えばいいじゃない\n","DEBUG = False\n","TRAIN = True\n","RUN_VALID = True\n","TOKENIZER = 'roberta-large'\n","PRETRAINED = 'roberta-large'\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1818,"status":"ok","timestamp":1643244384546,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"ZtARtCuEwFVx","outputId":"68007c26-6bbc-426c-880a-47ce1b0a6b3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["load data: this competition\n","load data: 1st competition\n"]}],"source":["# ----------------------------------------------\n","# Load Data\n","# ----------------------------------------------\n","# INPUT_0: This Competition\n","submission = pd.read_csv(INPUT_DIR_0/'sample_submission.csv')\n","val_data = pd.read_csv(INPUT_DIR_0/'validation_data.csv')\n","test = pd.read_csv(INPUT_DIR_0/'comments_to_score.csv')\n","print('load data: this competition')\n","\n","# INPUT_1: 1st Competition\n","train_1st = pd.read_csv(INPUT_DIR_1/'train.csv')\n","test_1st = pd.read_csv(INPUT_DIR_1/'test.csv')\n","test_labels_1st = pd.read_csv(INPUT_DIR_1/'test_labels.csv')\n","print('load data: 1st competition')\n","\n","# INPUT_2: 2nd Competition\n","#train_2nd = pd.read_csv(INPUT_DIR_2/'train.csv')\n","#test_2nd = pd.read_csv(INPUT_DIR_2/'test.csv')\n","#idt_indiv_anno = pd.read_csv(INPUT_DIR_2/'identity_individual_annotations.csv')\n","#tox_indiv_anno = pd.read_csv(INPUT_DIR_2/'toxicity_individual_annotations.csv')\n","#print('load data: 2nd competition')\n","\n","\n","# ----------------------------------------------\n","# Set SEED\n","# ----------------------------------------------\n","# seed\n","SEED = 2021\n","def set_seed(SEED):\n","    random.seed(SEED)\n","    np.random.seed(SEED)\n","    os.environ['PYTHONHASHSEED'] = str(SEED)\n","    \n","    torch.manual_seed(SEED)\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.deterministic = True\n","    \n","set_seed(SEED)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20165,"status":"ok","timestamp":1643244404707,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"_6_fFX6BwHEW","outputId":"bb6a8b91-2fef-4775-eff2-d8a070ef4fe2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sampling: Train\n","shape: (41003, 9)\n"]}],"source":["# ----------------------------------------------\n","# Sampling: Train_1st\n","# ----------------------------------------------\n","toxic_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","\n","test_1st_l = pd.merge(test_1st, test_labels_1st, on='id', how='left').query(\"toxic != -1\")\n","train_src = pd.concat([train_1st, test_1st_l], axis='rows')\n","train_src['target'] = train_src[toxic_cols].values.max(axis=1)\n","\n","sample_num = (train_src['target'] > 0).sum()\n","\n","train_tg0 = train_src[train_src['target'] == 0].sample(int(sample_num*1.0), random_state=SEED)\n","train_tg1 = train_src[train_src['target'] > 0]\n","train = pd.concat([train_tg0, train_tg1], axis='rows')\n","\n","val_comment_unq = pd.concat([val_data['less_toxic'], val_data['more_toxic']]).unique()\n","duplicate_idx = np.isin(train['comment_text'], val_comment_unq)\n","train = train.iloc[~duplicate_idx]\n","print('Sampling: Train')\n","print(f'shape: {train.shape}')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1643244404708,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"je3paKO8yzBa","outputId":"78ab045b-0bf4-40d3-ee8c-75feb39c6291"},"outputs":[{"data":{"text/plain":["0    22025\n","1    18978\n","Name: target, dtype: int64"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train.target.value_counts()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162,"referenced_widgets":["89a3875128f04b6bb3f9da07075cfa2a","daddad3168ec4417b88d1b5540dbb988","35fc87ab76334b90a3f0d97da036a46c","ffca3155db6640c6b702e3e359cdcf5e","631509f934c14e5697967765789b25cb","aa7645bf23524f2f96deccaa477158ae","6929eeb09fdd4a90a07a06cb6ed6a695","b59a37bdda8f40b5b3b7898476b7282f","1981b8953e1742e18efcccbe14247478","37d581aaec66426e9aa05aa0646b38df","363020e1794d4e07b4d5c80ac7193555","a667cceb10904e06a4966d2fe678f8b4","da1ecefce22940f1999dcd368cfd8d97","2f565f7a7d80413eae797df80c5239de","84ccb8910f7a44049189f786806ab1e9","6fde243cdb2f4dea95a41d3094d13d20","c09f024fd2c14d68899e94198333e526","e8f321f09ae842e8ab98507c092560c5","cacf3f098f704aa885248dbd77050272","4fda45dbabf048939077409930e20b09","e1c3af9774de4469877863d5aa158e4a","7d9d513b7e6c42f781d73a79329436c4","f48da8e6ea764c13931f74aeae51b4ef","6922f92c6c3340299afdcbe56448b4e0","1766844d82274992bd37dfb82c29b522","5bcfe34493d048f088509cdc02ed4adc","c5940f959a46463bb88d37428b14b4ec","4a4189f356fb452f960b5dc409aaa6fb","3b8dc34674b24e7e870547aabda9eab4","74f2e50bcbce48368d257260da4d7970","af3c276d42304e67a0a2091d8bbb2cbd","17be1c5155b3443fb9ad1dd10adac58a","b487892f01204e6997c50ef933c70073","1dcf0a58baf041e48886cd329fada235","4b0bef29b4df415e898a9975df8e158b","e8289a95e4054d02ba77f3f0d2ada610","ba784c458c0e46c48f8dc55ea8c1f460","8775646564d34a26acd2c4f6395f8e6c","29fc2f0dea5842679c5ed1c3652e5f4c","85e41e9a03d143e1921dc92208418cc7","5f72409ff8e34054beff83bc7528653e","e1e35a4ba15d48ef95a99867a7c9f020","d856f9f647e5493aab5590b88b652b9a","924ecb5c5dd54d679e635f68afd06958"]},"executionInfo":{"elapsed":2942,"status":"ok","timestamp":1643244407631,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"6rHp6VNuyMwU","outputId":"8c326095-710f-4eda-d410-ace36f20ddea"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89a3875128f04b6bb3f9da07075cfa2a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a667cceb10904e06a4966d2fe678f8b4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f48da8e6ea764c13931f74aeae51b4ef","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1dcf0a58baf041e48886cd329fada235","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["create: tokenizer\n"]}],"source":["# ----------------------------------------------\n","# Create Tokenizer\n","# ----------------------------------------------\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\n","print('create: tokenizer')\n","\n","\n","# ----------------------------------------------\n","# Preprocess func\n","# ----------------------------------------------\n","# Preprocess\n","import string\n","import re\n","import collections\n","from bs4 import BeautifulSoup\n","import nltk\n","#nltk.download('stopwords')\n","#nltk.download('averaged_perceptron_tagger')\n","\n","# https://www.kaggle.com/manabendrarout/pytorch-roberta-ranking-baseline-jrstc-train\n","def text_cleaning(text):\n","    '''\n","    Cleans text into a basic form for NLP. Operations include the following:-\n","    1. Remove special charecters like &, #, etc\n","    2. Removes extra spaces\n","    3. Removes embedded URL links\n","    4. Removes HTML tags\n","    5. Removes emojis\n","    \n","    text - Text piece to be cleaned.\n","    '''\n","    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n","    text = template.sub(r'', text)\n","    \n","    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n","    only_text = soup.get_text()\n","    text = only_text\n","    \n","    emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               u\"\\U00002702-\\U000027B0\"\n","                               u\"\\U000024C2-\\U0001F251\"\n","                               \"]+\", flags=re.UNICODE)\n","    text = emoji_pattern.sub(r'', text)\n","    \n","    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n","    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n","    text = text.strip() # remove spaces at the beginning and at the end of string\n","\n","    return text\n","\n","\n","def text_normalization(s:pd.Series):\n","    x = s.apply(text_cleaning)\n","    return x\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1643244407632,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"-NHRehe1yQoP"},"outputs":[],"source":["# ----------------------------------------------\n","# Dataset Class\n","# ----------------------------------------------\n","class Jigsaw1stDataset(Dataset):\n","    def __init__(self, df, inference_only=False):\n","        super().__init__\n","        \n","        self.df = df\n","        self.inference_only = inference_only\n","        \n","        if not self.inference_only:\n","            self.target = torch.tensor(df[toxic_cols].values, dtype=torch.float32)\n","        \n","        self.encoded = tokenizer.batch_encode_plus(\n","            text_normalization(df['comment_text']).tolist(),\n","            padding='max_length',\n","            max_length=MAX_LEN,\n","            truncation=True,\n","            return_attention_mask=True\n","        )\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        \n","        if self.inference_only:\n","            return {'input_ids': input_ids,\n","                    'attention_mask': attention_mask\n","                    }\n","        else:\n","            target = self.target[index]\n","            return {'input_ids': input_ids,\n","                    'attention_mask': attention_mask, \n","                    'target': target}\n","\n","\n","# ----------------------------------------------\n","# Model Class\n","# ----------------------------------------------\n","class AttentionHead(nn.Module):\n","    def __init__(self, in_features, hidden_dim, num_targets):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.middle_features = hidden_dim\n","        self.W = nn.Linear(in_features, hidden_dim)\n","        self.V = nn.Linear(hidden_dim, 1)\n","        self.out_features = hidden_dim\n","\n","    def forward(self, features):\n","        att = torch.tanh(self.W(features))\n","        score = self.V(att)\n","        attention_weights = torch.softmax(score, dim=1)\n","        context_vector = attention_weights * features\n","        context_vector = torch.sum(context_vector, dim=1)\n","\n","        return context_vector\n","\n","class Jigsaw1stModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        config = AutoConfig.from_pretrained(PRETRAINED)\n","        self.pre_model = AutoModel.from_pretrained(PRETRAINED)\n","        self.head = AttentionHead(config.hidden_size, config.hidden_size,1)\n","        self.dropout = nn.Dropout(0.3)\n","        self.regressor = nn.Linear(config.hidden_size, 6)\n","    \n","    def forward(self, input_ids, attention_mask):\n","        pre_out = self.pre_model(input_ids=input_ids, attention_mask=attention_mask)\n","        x0 = pre_out['last_hidden_state']\n","        x1 = self.head(x0)\n","        x2 = self.dropout(x1)\n","        x3 = self.regressor(x2)\n","        return x3"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1643244407633,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"DL-M9418cQ30"},"outputs":[],"source":["#j_ds = Jigsaw1stDataset(train)\n","#model = Jigsaw1stModel().to(DEVICE)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1643244407634,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"A6cZNZFRcJfS"},"outputs":[],"source":["#output = model(j_ds[:2]['input_ids'].to(DEVICE), j_ds[:2]['attention_mask'].to(DEVICE))"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1643244407634,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"7I5F2qs_cM7c"},"outputs":[],"source":["#output"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":405,"status":"ok","timestamp":1643244408027,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"NXCNVG2dEkpr"},"outputs":[],"source":["# ----------------------------------------------\n","# func: valid, predict\n","# ----------------------------------------------\n","def valid_mse(model, dataloader):\n","    model.eval()\n","    mse_sum = 0\n","    \n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(dataloader):\n","            input_ids = data['input_ids'].to(DEVICE)\n","            attention_mask = data['attention_mask'].to(DEVICE)\n","            target = data['target'].to(DEVICE)\n","            \n","            output = model(input_ids, attention_mask)\n","            \n","            mse_sum = nn.MSELoss(reduction='sum')(output.flatten(), target).item()\n","            \n","    return mse_sum/(len(dataloader.dataset))\n","\n","\n","def valid_bce(model, dataloader):\n","    model.eval()\n","    score_sum = 0\n","    \n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(dataloader):\n","            input_ids = data['input_ids'].to(DEVICE)\n","            attention_mask = data['attention_mask'].to(DEVICE)\n","            target = data['target'].to(DEVICE)\n","            \n","            output = model(input_ids, attention_mask)\n","            score_sum += nn.BCELoss(reduction='sum')(output.flatten(), target).item()\n","            \n","    return score_sum/(len(dataloader.dataset))\n","\n","def valid_bcelogit(model, dataloader):\n","    model.eval()\n","    score_sum = 0\n","    \n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(dataloader):\n","            input_ids = data['input_ids'].to(DEVICE)\n","            attention_mask = data['attention_mask'].to(DEVICE)\n","            target = data['target'].to(DEVICE)\n","            \n","            output = model(input_ids, attention_mask)\n","            score_sum += nn.BCEWithLogitsLoss(reduction='sum')(output, target).item()\n","            \n","    return score_sum/(len(dataloader.dataset)*target.shape[1])\n","\n","def predict(model, dataloader):\n","    model.eval()\n","    result = np.zeros((len(dataloader.dataset), 6))\n","    idx = 0\n","    \n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(dataloader):\n","            input_ids = data['input_ids'].to(DEVICE)\n","            attention_mask = data['attention_mask'].to(DEVICE)\n","            \n","            output = model(input_ids, attention_mask)\n","            result[idx:idx + output.shape[0], :] = output.to('cpu')\n","            \n","            idx += output.shape[0]\n","            \n","    return result\n","\n","\n","# ----------------------------------------------\n","# func: train\n","# ----------------------------------------------\n","def train_fn(\n","    model,\n","    save_path,\n","    train_loader,\n","    val_loader,\n","    optimizer,\n","    scheduler=None,\n","    num_epochs=NUM_EPOCHS\n","):\n","\n","    best_score = np.inf\n","    best_epoch = 0\n","    log_interval = 100 # TODO: 冒頭で設定する\n","\n","    start = time.time()\n","\n","    for epoch in range(num_epochs):\n","        val_score = None\n","\n","        for batch_idx, data in enumerate(train_loader):\n","            input_ids = data['input_ids'].to(DEVICE)\n","            attention_mask = data['attention_mask'].to(DEVICE)\n","            target = data['target'].to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            model.train()\n","\n","            output = model(input_ids, attention_mask)\n","            loss = nn.BCEWithLogitsLoss()(output, target)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            if scheduler:\n","                scheduler.step()\n","\n","            if (batch_idx > 0) & (batch_idx % log_interval == 0):\n","                val_score = valid_bcelogit(model, val_loader)\n","                print(f\"Epoch {epoch+1}, Step {batch_idx+1}, train_loss: {loss:0.5f}, val_loss: {val_score:0.5f}\")\n","                if val_score < best_score:\n","                    print(f\"Model Inproved: {best_score} ----> {val_score}\")\n","                    best_score = val_score\n","                    torch.save(model.state_dict(), save_path)\n","\n","            del input_ids\n","            del attention_mask\n","            del target\n","            del output\n","            torch.cuda.empty_cache()\n","\n","    print(f\"elasped time: {time.time() - start: 0.3}\")\n","    start = time.time()\n","\n","    return best_score\n","\n","\n","# ----------------------------------------------\n","# func: create optimizer\n","# ----------------------------------------------\n","def create_optimizer(model):\n","    named_params = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optim_params = []\n","    for idx_, (name_, params_) in enumerate(named_params):\n","        weight_decay = 0 if name_ in no_decay else 0.01\n","        optim_params.append({'params':params_,\n","                            'weight_decay': weight_decay,\n","                            #'lr': 1e-5\n","                            })\n","\n","    return AdamW(optim_params)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1eaa3bbcfdfe45be935e09e69cfe2074","7fa0a74a329c4e03837d5df60ea2510b","5266d5e0ac3841baa40fecae97e62942","ad291a93a81848fe8ab23ab3fbc80ce2","35da550d991f44ca90db5cc731a3a16b","9fae9696477d416bb7f9fde2c0888924","e0eeb0e1c36f43b1a265b2721de823ab","a0339dc76de9424495890ffa05936fbf","dfd1945830fe40c1bcf29e818981f76e","5922d970ec8a4371a9d201992eed2750","2c882efae546408f988446d248f66850"]},"executionInfo":{"elapsed":6570471,"status":"ok","timestamp":1643250978492,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"RlJqN5Jwv6zT","outputId":"8f30cdc8-c3c0-4c1d-f30c-9281af5b9982"},"outputs":[{"name":"stdout","output_type":"stream","text":["*** FOLD 1 / 5***\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1eaa3bbcfdfe45be935e09e69cfe2074","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Step 101, train_loss: 0.21463, val_loss: 0.18566\n","Model Inproved: inf ----> 0.18565587123729288\n","Epoch 1, Step 201, train_loss: 0.15968, val_loss: 0.15956\n","Model Inproved: 0.18565587123729288 ----> 0.15956111106766738\n","Epoch 1, Step 301, train_loss: 0.12397, val_loss: 0.14478\n","Model Inproved: 0.15956111106766738 ----> 0.14477552129345997\n","Epoch 1, Step 401, train_loss: 0.18689, val_loss: 0.15610\n","Epoch 1, Step 501, train_loss: 0.14095, val_loss: 0.14358\n","Model Inproved: 0.14477552129345997 ----> 0.14357956539773942\n","Epoch 1, Step 601, train_loss: 0.14462, val_loss: 0.13803\n","Model Inproved: 0.14357956539773942 ----> 0.13803072699452737\n","Epoch 1, Step 701, train_loss: 0.09218, val_loss: 0.15377\n","Epoch 1, Step 801, train_loss: 0.13090, val_loss: 0.14059\n","Epoch 1, Step 901, train_loss: 0.19561, val_loss: 0.14106\n","Epoch 1, Step 1001, train_loss: 0.20450, val_loss: 0.14214\n","Epoch 2, Step 101, train_loss: 0.09138, val_loss: 0.13782\n","Model Inproved: 0.13803072699452737 ----> 0.13782483334222112\n","Epoch 2, Step 201, train_loss: 0.11349, val_loss: 0.13780\n","Model Inproved: 0.13782483334222112 ----> 0.13780354881986284\n","Epoch 2, Step 301, train_loss: 0.11139, val_loss: 0.13911\n","Epoch 2, Step 401, train_loss: 0.10654, val_loss: 0.13576\n","Model Inproved: 0.13780354881986284 ----> 0.13575619510997095\n","Epoch 2, Step 501, train_loss: 0.10583, val_loss: 0.13606\n","Epoch 2, Step 601, train_loss: 0.14274, val_loss: 0.13336\n","Model Inproved: 0.13575619510997095 ----> 0.1333556778619034\n","Epoch 2, Step 701, train_loss: 0.09366, val_loss: 0.14073\n","Epoch 2, Step 801, train_loss: 0.07425, val_loss: 0.13680\n","Epoch 2, Step 901, train_loss: 0.14188, val_loss: 0.13415\n","Epoch 2, Step 1001, train_loss: 0.12471, val_loss: 0.13340\n","Epoch 3, Step 101, train_loss: 0.07185, val_loss: 0.13559\n","Epoch 3, Step 201, train_loss: 0.11840, val_loss: 0.13600\n","Epoch 3, Step 301, train_loss: 0.15911, val_loss: 0.13905\n","Epoch 3, Step 401, train_loss: 0.10666, val_loss: 0.13697\n","Epoch 3, Step 501, train_loss: 0.09799, val_loss: 0.13564\n","Epoch 3, Step 601, train_loss: 0.11861, val_loss: 0.13646\n","Epoch 3, Step 701, train_loss: 0.09698, val_loss: 0.13742\n","Epoch 3, Step 801, train_loss: 0.08566, val_loss: 0.13666\n","Epoch 3, Step 901, train_loss: 0.14098, val_loss: 0.13572\n","Epoch 3, Step 1001, train_loss: 0.09689, val_loss: 0.13557\n","elasped time:  1.28e+03\n","[0.1333556778619034]\n","Mean: 0.1333556778619034\n","*** FOLD 2 / 5***\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Step 101, train_loss: 0.29180, val_loss: 0.18114\n","Model Inproved: inf ----> 0.1811438584208503\n","Epoch 1, Step 201, train_loss: 0.13903, val_loss: 0.17233\n","Model Inproved: 0.1811438584208503 ----> 0.17232830025474022\n","Epoch 1, Step 301, train_loss: 0.09819, val_loss: 0.15589\n","Model Inproved: 0.17232830025474022 ----> 0.1558879045169586\n","Epoch 1, Step 401, train_loss: 0.10795, val_loss: 0.14489\n","Model Inproved: 0.1558879045169586 ----> 0.14489325632924197\n","Epoch 1, Step 501, train_loss: 0.14878, val_loss: 0.14040\n","Model Inproved: 0.14489325632924197 ----> 0.14040032844739594\n","Epoch 1, Step 601, train_loss: 0.20187, val_loss: 0.13663\n","Model Inproved: 0.14040032844739594 ----> 0.1366293041976357\n","Epoch 1, Step 701, train_loss: 0.12488, val_loss: 0.15726\n","Epoch 1, Step 801, train_loss: 0.17942, val_loss: 0.13429\n","Model Inproved: 0.1366293041976357 ----> 0.1342884589018281\n","Epoch 1, Step 901, train_loss: 0.12926, val_loss: 0.13612\n","Epoch 1, Step 1001, train_loss: 0.14740, val_loss: 0.13668\n","Epoch 2, Step 101, train_loss: 0.10285, val_loss: 0.13411\n","Model Inproved: 0.1342884589018281 ----> 0.13411113003116892\n","Epoch 2, Step 201, train_loss: 0.09006, val_loss: 0.13327\n","Model Inproved: 0.13411113003116892 ----> 0.13326633977331834\n","Epoch 2, Step 301, train_loss: 0.13093, val_loss: 0.13441\n","Epoch 2, Step 401, train_loss: 0.15519, val_loss: 0.13339\n","Epoch 2, Step 501, train_loss: 0.14205, val_loss: 0.13405\n","Epoch 2, Step 601, train_loss: 0.08467, val_loss: 0.13267\n","Model Inproved: 0.13326633977331834 ----> 0.1326693105829909\n","Epoch 2, Step 701, train_loss: 0.10117, val_loss: 0.14167\n","Epoch 2, Step 801, train_loss: 0.13153, val_loss: 0.13223\n","Model Inproved: 0.1326693105829909 ----> 0.13223457893152496\n","Epoch 2, Step 901, train_loss: 0.11019, val_loss: 0.13066\n","Model Inproved: 0.13223457893152496 ----> 0.1306646707491318\n","Epoch 2, Step 1001, train_loss: 0.10129, val_loss: 0.13112\n","Epoch 3, Step 101, train_loss: 0.14125, val_loss: 0.13433\n","Epoch 3, Step 201, train_loss: 0.09105, val_loss: 0.13544\n","Epoch 3, Step 301, train_loss: 0.07818, val_loss: 0.13592\n","Epoch 3, Step 401, train_loss: 0.09145, val_loss: 0.13548\n","Epoch 3, Step 501, train_loss: 0.15345, val_loss: 0.13418\n","Epoch 3, Step 601, train_loss: 0.04285, val_loss: 0.13626\n","Epoch 3, Step 701, train_loss: 0.09736, val_loss: 0.13549\n","Epoch 3, Step 801, train_loss: 0.06733, val_loss: 0.13521\n","Epoch 3, Step 901, train_loss: 0.12016, val_loss: 0.13540\n","Epoch 3, Step 1001, train_loss: 0.12923, val_loss: 0.13512\n","elasped time:  1.27e+03\n","[0.1333556778619034, 0.1306646707491318]\n","Mean: 0.1320101743055176\n","*** FOLD 3 / 5***\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Step 101, train_loss: 0.17725, val_loss: 0.17646\n","Model Inproved: inf ----> 0.1764550816405669\n","Epoch 1, Step 201, train_loss: 0.15695, val_loss: 0.19812\n","Epoch 1, Step 301, train_loss: 0.11968, val_loss: 0.16225\n","Model Inproved: 0.1764550816405669 ----> 0.16224818410308642\n","Epoch 1, Step 401, train_loss: 0.15798, val_loss: 0.15140\n","Model Inproved: 0.16224818410308642 ----> 0.1514015944150484\n","Epoch 1, Step 501, train_loss: 0.08634, val_loss: 0.14084\n","Model Inproved: 0.1514015944150484 ----> 0.14084188569800007\n","Epoch 1, Step 601, train_loss: 0.16986, val_loss: 0.14607\n","Epoch 1, Step 701, train_loss: 0.12018, val_loss: 0.14214\n","Epoch 1, Step 801, train_loss: 0.11741, val_loss: 0.14623\n","Epoch 1, Step 901, train_loss: 0.12355, val_loss: 0.13975\n","Model Inproved: 0.14084188569800007 ----> 0.13975296448639862\n","Epoch 1, Step 1001, train_loss: 0.17958, val_loss: 0.13402\n","Model Inproved: 0.13975296448639862 ----> 0.1340237859787507\n","Epoch 2, Step 101, train_loss: 0.12139, val_loss: 0.13948\n","Epoch 2, Step 201, train_loss: 0.19480, val_loss: 0.14604\n","Epoch 2, Step 301, train_loss: 0.14549, val_loss: 0.13640\n","Epoch 2, Step 401, train_loss: 0.13380, val_loss: 0.14283\n","Epoch 2, Step 501, train_loss: 0.10746, val_loss: 0.14985\n","Epoch 2, Step 601, train_loss: 0.11368, val_loss: 0.13689\n","Epoch 2, Step 701, train_loss: 0.16774, val_loss: 0.14054\n","Epoch 2, Step 801, train_loss: 0.08418, val_loss: 0.13731\n","Epoch 2, Step 901, train_loss: 0.12595, val_loss: 0.14372\n","Epoch 2, Step 1001, train_loss: 0.13620, val_loss: 0.13906\n","Epoch 3, Step 101, train_loss: 0.14736, val_loss: 0.13530\n","Epoch 3, Step 201, train_loss: 0.16349, val_loss: 0.13425\n","Epoch 3, Step 301, train_loss: 0.13139, val_loss: 0.13510\n","Epoch 3, Step 401, train_loss: 0.09624, val_loss: 0.13842\n","Epoch 3, Step 501, train_loss: 0.15232, val_loss: 0.13645\n","Epoch 3, Step 601, train_loss: 0.05903, val_loss: 0.13822\n","Epoch 3, Step 701, train_loss: 0.10370, val_loss: 0.13587\n","Epoch 3, Step 801, train_loss: 0.10301, val_loss: 0.13646\n","Epoch 3, Step 901, train_loss: 0.11255, val_loss: 0.13755\n","Epoch 3, Step 1001, train_loss: 0.07050, val_loss: 0.13718\n","elasped time:  1.26e+03\n","[0.1333556778619034, 0.1306646707491318, 0.1340237859787507]\n","Mean: 0.13268137819659528\n","*** FOLD 4 / 5***\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Step 101, train_loss: 0.21068, val_loss: 0.16419\n","Model Inproved: inf ----> 0.16418881987653128\n","Epoch 1, Step 201, train_loss: 0.21711, val_loss: 0.15035\n","Model Inproved: 0.16418881987653128 ----> 0.15035124030297364\n","Epoch 1, Step 301, train_loss: 0.18702, val_loss: 0.15429\n","Epoch 1, Step 401, train_loss: 0.19129, val_loss: 0.14626\n","Model Inproved: 0.15035124030297364 ----> 0.14625648929336207\n","Epoch 1, Step 501, train_loss: 0.09235, val_loss: 0.14193\n","Model Inproved: 0.14625648929336207 ----> 0.14193191853238316\n","Epoch 1, Step 601, train_loss: 0.18864, val_loss: 0.13406\n","Model Inproved: 0.14193191853238316 ----> 0.13405966470881206\n","Epoch 1, Step 701, train_loss: 0.11372, val_loss: 0.14358\n","Epoch 1, Step 801, train_loss: 0.13997, val_loss: 0.13498\n","Epoch 1, Step 901, train_loss: 0.13774, val_loss: 0.13318\n","Model Inproved: 0.13405966470881206 ----> 0.13317955645603863\n","Epoch 1, Step 1001, train_loss: 0.13184, val_loss: 0.14831\n","Epoch 2, Step 101, train_loss: 0.16567, val_loss: 0.13004\n","Model Inproved: 0.13317955645603863 ----> 0.13003667323812237\n","Epoch 2, Step 201, train_loss: 0.18523, val_loss: 0.14070\n","Epoch 2, Step 301, train_loss: 0.11889, val_loss: 0.12940\n","Model Inproved: 0.13003667323812237 ----> 0.1294038041093485\n","Epoch 2, Step 401, train_loss: 0.09382, val_loss: 0.13202\n","Epoch 2, Step 501, train_loss: 0.14631, val_loss: 0.13204\n","Epoch 2, Step 601, train_loss: 0.12276, val_loss: 0.13677\n","Epoch 2, Step 701, train_loss: 0.08407, val_loss: 0.13253\n","Epoch 2, Step 801, train_loss: 0.15970, val_loss: 0.12896\n","Model Inproved: 0.1294038041093485 ----> 0.12896424471241671\n","Epoch 2, Step 901, train_loss: 0.10790, val_loss: 0.13201\n","Epoch 2, Step 1001, train_loss: 0.10685, val_loss: 0.13262\n","Epoch 3, Step 101, train_loss: 0.04629, val_loss: 0.13063\n","Epoch 3, Step 201, train_loss: 0.10908, val_loss: 0.13124\n","Epoch 3, Step 301, train_loss: 0.12467, val_loss: 0.13080\n","Epoch 3, Step 401, train_loss: 0.12187, val_loss: 0.13137\n","Epoch 3, Step 501, train_loss: 0.13273, val_loss: 0.13270\n","Epoch 3, Step 601, train_loss: 0.17874, val_loss: 0.13006\n","Epoch 3, Step 701, train_loss: 0.19904, val_loss: 0.13206\n","Epoch 3, Step 801, train_loss: 0.09352, val_loss: 0.13141\n","Epoch 3, Step 901, train_loss: 0.12356, val_loss: 0.13142\n","Epoch 3, Step 1001, train_loss: 0.08562, val_loss: 0.13145\n","elasped time:  1.27e+03\n","[0.1333556778619034, 0.1306646707491318, 0.1340237859787507, 0.12896424471241671]\n","Mean: 0.13175209482555064\n","*** FOLD 5 / 5***\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Step 101, train_loss: 0.16076, val_loss: 0.18052\n","Model Inproved: inf ----> 0.1805243036320539\n","Epoch 1, Step 201, train_loss: 0.16575, val_loss: 0.15898\n","Model Inproved: 0.1805243036320539 ----> 0.15897566652394893\n","Epoch 1, Step 301, train_loss: 0.10934, val_loss: 0.14901\n","Model Inproved: 0.15897566652394893 ----> 0.1490100239447462\n","Epoch 1, Step 401, train_loss: 0.10063, val_loss: 0.14392\n","Model Inproved: 0.1490100239447462 ----> 0.14392138160220005\n","Epoch 1, Step 501, train_loss: 0.08678, val_loss: 0.14755\n","Epoch 1, Step 601, train_loss: 0.12683, val_loss: 0.15227\n","Epoch 1, Step 701, train_loss: 0.17480, val_loss: 0.14562\n","Epoch 1, Step 801, train_loss: 0.16728, val_loss: 0.14335\n","Model Inproved: 0.14392138160220005 ----> 0.1433517573458877\n","Epoch 1, Step 901, train_loss: 0.13331, val_loss: 0.14049\n","Model Inproved: 0.1433517573458877 ----> 0.14048946482136968\n","Epoch 1, Step 1001, train_loss: 0.16676, val_loss: 0.13846\n","Model Inproved: 0.14048946482136968 ----> 0.13846309527633635\n","Epoch 2, Step 101, train_loss: 0.13038, val_loss: 0.14324\n","Epoch 2, Step 201, train_loss: 0.09663, val_loss: 0.13986\n","Epoch 2, Step 301, train_loss: 0.13826, val_loss: 0.14163\n","Epoch 2, Step 401, train_loss: 0.11371, val_loss: 0.14175\n","Epoch 2, Step 501, train_loss: 0.20714, val_loss: 0.13270\n","Model Inproved: 0.13846309527633635 ----> 0.13270263923256378\n","Epoch 2, Step 601, train_loss: 0.13939, val_loss: 0.13259\n","Model Inproved: 0.13270263923256378 ----> 0.13258529060436944\n","Epoch 2, Step 701, train_loss: 0.13131, val_loss: 0.13691\n","Epoch 2, Step 801, train_loss: 0.14051, val_loss: 0.13337\n","Epoch 2, Step 901, train_loss: 0.10909, val_loss: 0.13695\n","Epoch 2, Step 1001, train_loss: 0.13097, val_loss: 0.13480\n","Epoch 3, Step 101, train_loss: 0.11244, val_loss: 0.13971\n","Epoch 3, Step 201, train_loss: 0.09037, val_loss: 0.13851\n","Epoch 3, Step 301, train_loss: 0.16398, val_loss: 0.13557\n","Epoch 3, Step 401, train_loss: 0.16853, val_loss: 0.13593\n","Epoch 3, Step 501, train_loss: 0.15462, val_loss: 0.13430\n","Epoch 3, Step 601, train_loss: 0.08844, val_loss: 0.13441\n","Epoch 3, Step 701, train_loss: 0.12695, val_loss: 0.13475\n","Epoch 3, Step 801, train_loss: 0.10258, val_loss: 0.13342\n","Epoch 3, Step 901, train_loss: 0.10440, val_loss: 0.13340\n","Epoch 3, Step 1001, train_loss: 0.11570, val_loss: 0.13350\n","elasped time:  1.28e+03\n","[0.1333556778619034, 0.1306646707491318, 0.1340237859787507, 0.12896424471241671, 0.13258529060436944]\n","Mean: 0.1319187339813144\n"]}],"source":["# ----------------------------------------------\n","# Main Loop\n","# ----------------------------------------------\n","if TRAIN:\n","    val_scores = []\n","    from sklearn.model_selection import KFold\n","    kfold = KFold(n_splits=FOLDS, random_state=SEED, shuffle=True)\n","\n","    for fold, (train_idx, val_idx) in enumerate(kfold.split(train)): \n","        print(f\"*** FOLD {fold+1} / {FOLDS}***\")\n","\n","        save_path = f\"/content/model/model_{fold+1}.pth\"\n","\n","        train_set = Jigsaw1stDataset(train.iloc[train_idx])\n","        valid_set = Jigsaw1stDataset(train.iloc[val_idx])\n","\n","        train_loader = DataLoader(train_set,\n","                                batch_size=BATCH_SIZE,\n","                                shuffle=True,\n","                                drop_last=True,\n","                                num_workers=2)\n","        valid_loader = DataLoader(valid_set,\n","                                batch_size=BATCH_SIZE,\n","                                shuffle=False,\n","                                drop_last=False,\n","                                num_workers=2)\n","\n","        model = Jigsaw1stModel().to(DEVICE)\n","        optimizer = AdamW(model.parameters(), lr=LEANING_RATE)\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_training_steps=NUM_EPOCHS*len(train_loader),\n","            num_warmup_steps=50\n","        )\n","\n","        val_scores.append(\n","            train_fn(model, save_path, train_loader, valid_loader, optimizer, scheduler=scheduler)\n","        )\n","\n","        del model\n","        torch.cuda.empty_cache()\n","\n","        print(val_scores)\n","        print(\"Mean:\", np.array(val_scores).mean())"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1643251046712,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"EZvKXLkZQGOm"},"outputs":[],"source":["def predict(model, dataloader):\n","    model.eval()\n","    result = np.zeros((len(dataloader.dataset), 6))\n","    idx = 0\n","    \n","    with torch.no_grad():\n","        for batch_idx, data in enumerate(dataloader):\n","            input_ids = data['input_ids'].to(DEVICE)\n","            attention_mask = data['attention_mask'].to(DEVICE)\n","            \n","            output = model(input_ids, attention_mask)\n","            result[idx:idx + output.shape[0], :] = output.to('cpu')\n","            \n","            idx += output.shape[0]\n","            \n","    return result"]},{"cell_type":"markdown","metadata":{"id":"QRp8W2zmMnit"},"source":["# Predict or Load Valid data"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1643251156031,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"PmvFLO0EMxDR","outputId":"995f7029-66a3-4d8a-ac5c-15b59558ca27"},"outputs":[{"name":"stdout","output_type":"stream","text":["['/content/model/model_1.pth', '/content/model/model_2.pth', '/content/model/model_3.pth', '/content/model/model_4.pth', '/content/model/model_5.pth']\n"]}],"source":["model_path = UPLOAD_DIR\n","models = sorted([str(i) for i in list(model_path.iterdir())])[1:]\n","print(models)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1643251156032,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"RPss9RpKcnyt"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","scaler_ = MinMaxScaler()\n","oof_sc = scaler_.fit_transform(oof_preds)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1643251156032,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"pOQf_fo_L_sk"},"outputs":[],"source":["# val: less, more別々にdf作成 -> スコアを予測し、「more > less」である率を計測する。\n","val_less = val_data[['less_toxic']].rename({'less_toxic': 'comment_text'}, axis='columns')\n","val_more = val_data[['more_toxic']].rename({'more_toxic': 'comment_text'}, axis='columns')"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1643251156032,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"XExFn5jSL_ck"},"outputs":[],"source":["def calc_val(model, model_path, less_dataset, more_dataset):\n","    less_pred = np.zeros((FOLDS, len(less_dataset), 6))\n","    more_pred = np.zeros((FOLDS, len(more_dataset), 6))\n","\n","    less_loader = DataLoader(less_dataset, batch_size=BATCH_SIZE,\n","                             drop_last=False, shuffle=False, num_workers=2)\n","    more_loader = DataLoader(more_dataset, batch_size=BATCH_SIZE,\n","                             drop_last=False, shuffle=False, num_workers=2)\n","\n","    for i, model_ in enumerate(model_path):\n","        print(f\"model-{i}: start\")\n","\n","        model.to(DEVICE)\n","        model.load_state_dict(torch.load(model_))\n","\n","        less_pred[i, :] = predict(model, less_loader)\n","        more_pred[i, :] = predict(model, more_loader)\n","        print(f\"model-{i}: complete\")\n","\n","    less_mean = less_pred.mean(axis=0)\n","    #less_mean = scaler_.transform(less_mean)\n","    more_mean = more_pred.mean(axis=0)\n","    #more_mean = scaler_.transform(more_mean)\n","\n","    val_scores = pd.DataFrame({'worker': val_data['worker'].head(len(less_dataset)),\n","                               'less_score': less_mean.sum(axis=1),\n","                               'more_score': more_mean.sum(axis=1),})\n","    val_scores['score_diff'] = val_scores['more_score'] - val_scores['less_score']\n","    val_scores['correct_ans'] = val_scores['score_diff'] > 0\n","\n","    acc = val_scores['correct_ans'].sum() / len(val_scores)\n","\n","    print(f\"accuracy: {acc}\")\n","    print(f\"{val_scores['correct_ans'].sum()} / {len(val_scores)}\")\n","    return less_mean, more_mean, val_scores"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491637,"status":"ok","timestamp":1643251712269,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"Wx9W2oAsPgHs","outputId":"b6d68fe8-38a3-4b5a-fa66-7d29de606d22"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["model-0: start\n","model-0: complete\n","model-1: start\n","model-1: complete\n","model-2: start\n","model-2: complete\n","model-3: start\n","model-3: complete\n","model-4: start\n","model-4: complete\n","accuracy: 0.7031021655373987\n","21169 / 30108\n"]}],"source":["if RUN_VALID:\n","  model = Jigsaw1stModel()\n","  less_dataset = Jigsaw1stDataset(val_less, inference_only=True)\n","  more_dataset = Jigsaw1stDataset(val_more, inference_only=True)\n","\n","  less_, more_, scores = calc_val(model, models, less_dataset, more_dataset)\n","  pd.DataFrame(less_).to_csv(out_path/'less_df.csv', index=False)\n","  pd.DataFrame(more_).to_csv(out_path/'more_df.csv', index=False)\n","  scores.to_csv(out_path/'out_score.csv', index=False)\n","  scores.head()\n","\n","else:\n","  less_df = pd.read_csv(out_path/'less_df.csv')\n","  more_df = pd.read_csv(out_path/'more_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nneeudNvZ71g"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMN/54skzUO08DpCKaf4J2P","background_execution":"on","collapsed_sections":[],"mount_file_id":"1E7OC1O__qj14BzVZHDsIYgqoOEh6PfDP","name":"calpis-001.ipynb","provenance":[{"file_id":"1OTfi82QAmeROtNWaS1ykG8ROVUvG9TDK","timestamp":1643155772647},{"file_id":"1teetF6U9uIL5kT0wNc0fH4AGRjX8Icux","timestamp":1643098853642},{"file_id":"1fBWjLs_VWGdtCm8I7NOA9BNixWqigUiK","timestamp":1638329371432},{"file_id":"19kyMmv8MoUznXvQ7pEVv7DIpVhGfroxO","timestamp":1638148434790},{"file_id":"1m2opANSAu0f6JSVhVAGXiULprzaG2BA5","timestamp":1638071812320},{"file_id":"1WL8hIv_RgQrxzWiQz_YLbalnY7yF5x4d","timestamp":1637753999122},{"file_id":"1gJFOrI0qy1Q3ilk5t6MWn68czi81mJWX","timestamp":1637557926622},{"file_id":"1AGKdYWEJiSyvxeY-Teo0i-hm7UI8ucSV","timestamp":1637502066835}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1766844d82274992bd37dfb82c29b522":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b8dc34674b24e7e870547aabda9eab4","placeholder":"​","style":"IPY_MODEL_4a4189f356fb452f960b5dc409aaa6fb","value":"Downloading: 100%"}},"17be1c5155b3443fb9ad1dd10adac58a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1981b8953e1742e18efcccbe14247478":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dcf0a58baf041e48886cd329fada235":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e8289a95e4054d02ba77f3f0d2ada610","IPY_MODEL_ba784c458c0e46c48f8dc55ea8c1f460","IPY_MODEL_8775646564d34a26acd2c4f6395f8e6c"],"layout":"IPY_MODEL_4b0bef29b4df415e898a9975df8e158b"}},"1eaa3bbcfdfe45be935e09e69cfe2074":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5266d5e0ac3841baa40fecae97e62942","IPY_MODEL_ad291a93a81848fe8ab23ab3fbc80ce2","IPY_MODEL_35da550d991f44ca90db5cc731a3a16b"],"layout":"IPY_MODEL_7fa0a74a329c4e03837d5df60ea2510b"}},"29fc2f0dea5842679c5ed1c3652e5f4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c882efae546408f988446d248f66850":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f565f7a7d80413eae797df80c5239de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8f321f09ae842e8ab98507c092560c5","placeholder":"​","style":"IPY_MODEL_c09f024fd2c14d68899e94198333e526","value":"Downloading: 100%"}},"35da550d991f44ca90db5cc731a3a16b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c882efae546408f988446d248f66850","placeholder":"​","style":"IPY_MODEL_5922d970ec8a4371a9d201992eed2750","value":" 1.33G/1.33G [00:24&lt;00:00, 56.7MB/s]"}},"35fc87ab76334b90a3f0d97da036a46c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6929eeb09fdd4a90a07a06cb6ed6a695","placeholder":"​","style":"IPY_MODEL_aa7645bf23524f2f96deccaa477158ae","value":"Downloading: 100%"}},"363020e1794d4e07b4d5c80ac7193555":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37d581aaec66426e9aa05aa0646b38df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b8dc34674b24e7e870547aabda9eab4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a4189f356fb452f960b5dc409aaa6fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b0bef29b4df415e898a9975df8e158b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fda45dbabf048939077409930e20b09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5266d5e0ac3841baa40fecae97e62942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0eeb0e1c36f43b1a265b2721de823ab","placeholder":"​","style":"IPY_MODEL_9fae9696477d416bb7f9fde2c0888924","value":"Downloading: 100%"}},"5922d970ec8a4371a9d201992eed2750":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bcfe34493d048f088509cdc02ed4adc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af3c276d42304e67a0a2091d8bbb2cbd","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74f2e50bcbce48368d257260da4d7970","value":456318}},"5f72409ff8e34054beff83bc7528653e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"631509f934c14e5697967765789b25cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_363020e1794d4e07b4d5c80ac7193555","placeholder":"​","style":"IPY_MODEL_37d581aaec66426e9aa05aa0646b38df","value":" 482/482 [00:00&lt;00:00, 19.0kB/s]"}},"6922f92c6c3340299afdcbe56448b4e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6929eeb09fdd4a90a07a06cb6ed6a695":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fde243cdb2f4dea95a41d3094d13d20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d9d513b7e6c42f781d73a79329436c4","placeholder":"​","style":"IPY_MODEL_e1c3af9774de4469877863d5aa158e4a","value":" 878k/878k [00:00&lt;00:00, 5.68MB/s]"}},"74f2e50bcbce48368d257260da4d7970":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d9d513b7e6c42f781d73a79329436c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fa0a74a329c4e03837d5df60ea2510b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84ccb8910f7a44049189f786806ab1e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fda45dbabf048939077409930e20b09","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cacf3f098f704aa885248dbd77050272","value":898823}},"85e41e9a03d143e1921dc92208418cc7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8775646564d34a26acd2c4f6395f8e6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_924ecb5c5dd54d679e635f68afd06958","placeholder":"​","style":"IPY_MODEL_d856f9f647e5493aab5590b88b652b9a","value":" 1.29M/1.29M [00:00&lt;00:00, 6.93MB/s]"}},"89a3875128f04b6bb3f9da07075cfa2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35fc87ab76334b90a3f0d97da036a46c","IPY_MODEL_ffca3155db6640c6b702e3e359cdcf5e","IPY_MODEL_631509f934c14e5697967765789b25cb"],"layout":"IPY_MODEL_daddad3168ec4417b88d1b5540dbb988"}},"924ecb5c5dd54d679e635f68afd06958":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fae9696477d416bb7f9fde2c0888924":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0339dc76de9424495890ffa05936fbf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a667cceb10904e06a4966d2fe678f8b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f565f7a7d80413eae797df80c5239de","IPY_MODEL_84ccb8910f7a44049189f786806ab1e9","IPY_MODEL_6fde243cdb2f4dea95a41d3094d13d20"],"layout":"IPY_MODEL_da1ecefce22940f1999dcd368cfd8d97"}},"aa7645bf23524f2f96deccaa477158ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad291a93a81848fe8ab23ab3fbc80ce2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfd1945830fe40c1bcf29e818981f76e","max":1425941629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0339dc76de9424495890ffa05936fbf","value":1425941629}},"af3c276d42304e67a0a2091d8bbb2cbd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b487892f01204e6997c50ef933c70073":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b59a37bdda8f40b5b3b7898476b7282f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba784c458c0e46c48f8dc55ea8c1f460":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1e35a4ba15d48ef95a99867a7c9f020","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f72409ff8e34054beff83bc7528653e","value":1355863}},"c09f024fd2c14d68899e94198333e526":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5940f959a46463bb88d37428b14b4ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b487892f01204e6997c50ef933c70073","placeholder":"​","style":"IPY_MODEL_17be1c5155b3443fb9ad1dd10adac58a","value":" 446k/446k [00:00&lt;00:00, 4.63MB/s]"}},"cacf3f098f704aa885248dbd77050272":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d856f9f647e5493aab5590b88b652b9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da1ecefce22940f1999dcd368cfd8d97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daddad3168ec4417b88d1b5540dbb988":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfd1945830fe40c1bcf29e818981f76e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0eeb0e1c36f43b1a265b2721de823ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1c3af9774de4469877863d5aa158e4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1e35a4ba15d48ef95a99867a7c9f020":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8289a95e4054d02ba77f3f0d2ada610":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85e41e9a03d143e1921dc92208418cc7","placeholder":"​","style":"IPY_MODEL_29fc2f0dea5842679c5ed1c3652e5f4c","value":"Downloading: 100%"}},"e8f321f09ae842e8ab98507c092560c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f48da8e6ea764c13931f74aeae51b4ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1766844d82274992bd37dfb82c29b522","IPY_MODEL_5bcfe34493d048f088509cdc02ed4adc","IPY_MODEL_c5940f959a46463bb88d37428b14b4ec"],"layout":"IPY_MODEL_6922f92c6c3340299afdcbe56448b4e0"}},"ffca3155db6640c6b702e3e359cdcf5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1981b8953e1742e18efcccbe14247478","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b59a37bdda8f40b5b3b7898476b7282f","value":482}}}}},"nbformat":4,"nbformat_minor":0}
